{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2863a14390>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f28639ea198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "读取MNIST数据\n",
    "\"\"\"\n",
    "\n",
    "def load_data():\n",
    "\n",
    "    img_test_data = open('GANs_MNIST/t10k-images.idx3-ubyte')\n",
    "    test_label = open('GANs_MNIST/t10k-labels.idx1-ubyte')\n",
    "    img_train_data = open('GANs_MNIST/train-images.idx3-ubyte')\n",
    "    train_label = open('GANs_MNIST/train-labels.idx1-ubyte')\n",
    "\n",
    "    num_test = 10000\n",
    "    num_train = 60000\n",
    "    length = 28 * 28\n",
    "\n",
    "    array_test_data = np.fromfile(img_test_data, dtype=np.uint8)\n",
    "    array_test_label = np.fromfile(test_label, dtype=np.uint8)\n",
    "    array_train_data = np.fromfile(img_train_data, dtype=np.uint8)\n",
    "    array_train_label = np.fromfile(train_label, dtype=np.uint8)\n",
    "    \n",
    "#     t_imgs = np.transpose(array_test_data[16:].reshape(28, 28, 10000), (2, 0, 1))\n",
    "#     t_labels = array_test_label[8:]\n",
    "#     tr_imgs = np.transpose(array_train_data[16:].reshape(28, 28, 60000), (2, 0, 1))\n",
    "#     tr_labels = array_train_label[8:]\n",
    "\n",
    "    t_imgs = []\n",
    "    t_labels = array_test_label[8:]\n",
    "    tr_imgs = []\n",
    "    tr_labels = array_train_label[8:]\n",
    "\n",
    "    for i in range(num_test):\n",
    "        t_imgs.append(array_test_data[16 + i * length : 16 + (i + 1) * length])\n",
    "\n",
    "    for i in range(num_train):\n",
    "        tr_imgs.append(array_train_data[16 + i * length : 16 + (i + 1) * length])\n",
    "        \n",
    "    tr_imgs = np.array(tr_imgs)\n",
    "    t_imgs = np.array(t_imgs)\n",
    "    \n",
    "    return (tr_imgs, tr_labels), (t_imgs, t_labels)\n",
    "\n",
    "(tr_i, tr_l), (t_i, t_l) = load_data()\n",
    "print(tr_l[4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import math\n",
    "\n",
    "\n",
    "def generator_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim=100, output_dim=1024))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(128*7*7))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Reshape((7, 7, 128), input_shape=(128*7*7,)))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Conv2D(64, (5, 5), padding='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Conv2D(1, (5, 5), padding='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def discriminator_model():\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "            Conv2D(64, (5, 5),\n",
    "            padding='same',\n",
    "            input_shape=(28, 28, 1))\n",
    "            )\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(128, (5, 5)))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def generator_containing_discriminator(g, d):\n",
    "    model = Sequential()\n",
    "    model.add(g)\n",
    "    d.trainable = False\n",
    "    model.add(d)\n",
    "    return model\n",
    "\n",
    "\n",
    "def combine_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num)/width))\n",
    "    shape = generated_images.shape[1:3]\n",
    "    image = np.zeros((height*shape[0], width*shape[1]),\n",
    "                     dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index/width)\n",
    "        j = index % width\n",
    "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
    "            img[:, :, 0]\n",
    "    return image\n",
    "\n",
    "\n",
    "def train(BATCH_SIZE):\n",
    "    (X_train, y_train), (X_test, y_test) = load_data()\n",
    "    X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
    "    X_train = X_train[:, :, :, None]\n",
    "    X_test = X_test[:, :, :, None]\n",
    "    # X_train = X_train.reshape((X_train.shape, 1) + X_train.shape[1:])\n",
    "    d = discriminator_model()\n",
    "    g = generator_model()\n",
    "    d_on_g = generator_containing_discriminator(g, d)\n",
    "    d_optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "    g_optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "    g.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "    d_on_g.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
    "    d.trainable = True\n",
    "    d.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
    "    for epoch in range(100):\n",
    "        print(\"Epoch is\", epoch)\n",
    "        print(\"Number of batches\", int(X_train.shape[0]/BATCH_SIZE))\n",
    "        for index in range(int(X_train.shape[0]/BATCH_SIZE)):\n",
    "            noise = np.random.uniform(-1, 1, size=(BATCH_SIZE, 100))\n",
    "            image_batch = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
    "            generated_images = g.predict(noise, verbose=0)\n",
    "            if index % 20 == 0:\n",
    "                image = combine_images(generated_images)\n",
    "                image = image*127.5+127.5\n",
    "                Image.fromarray(image.astype(np.uint8)).save(\n",
    "                    str(epoch)+\"_\"+str(index)+\".png\")\n",
    "            X = np.concatenate((image_batch, generated_images))\n",
    "            y = [1] * BATCH_SIZE + [0] * BATCH_SIZE\n",
    "            d_loss = d.train_on_batch(X, y)\n",
    "            print(\"batch %d d_loss : %f\" % (index, d_loss))\n",
    "            noise = np.random.uniform(-1, 1, (BATCH_SIZE, 100))\n",
    "            d.trainable = False\n",
    "            g_loss = d_on_g.train_on_batch(noise, [1] * BATCH_SIZE)\n",
    "            d.trainable = True\n",
    "            print(\"batch %d g_loss : %f\" % (index, g_loss))\n",
    "            if index % 10 == 9:\n",
    "                g.save_weights('generator', True)\n",
    "                d.save_weights('discriminator', True)\n",
    "\n",
    "\n",
    "def generate(BATCH_SIZE, nice=False):\n",
    "    g = generator_model()\n",
    "    g.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "    g.load_weights('generator')\n",
    "    if nice:\n",
    "        d = discriminator_model()\n",
    "        d.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "        d.load_weights('discriminator')\n",
    "        noise = np.random.uniform(-1, 1, (BATCH_SIZE*20, 100))\n",
    "        generated_images = g.predict(noise, verbose=1)\n",
    "        d_pret = d.predict(generated_images, verbose=1)\n",
    "        index = np.arange(0, BATCH_SIZE*20)\n",
    "        index.resize((BATCH_SIZE*20, 1))\n",
    "        pre_with_index = list(np.append(d_pret, index, axis=1))\n",
    "        pre_with_index.sort(key=lambda x: x[0], reverse=True)\n",
    "        nice_images = np.zeros((BATCH_SIZE,) + generated_images.shape[1:3], dtype=np.float32)\n",
    "        nice_images = nice_images[:, :, :, None]\n",
    "        for i in range(BATCH_SIZE):\n",
    "            idx = int(pre_with_index[i][1])\n",
    "            nice_images[i, :, :, 0] = generated_images[idx, :, :, 0]\n",
    "        image = combine_images(nice_images)\n",
    "    else:\n",
    "        noise = np.random.uniform(-1, 1, (BATCH_SIZE, 100))\n",
    "        generated_images = g.predict(noise, verbose=1)\n",
    "        image = combine_images(generated_images)\n",
    "    image = image*127.5+127.5\n",
    "    Image.fromarray(image.astype(np.uint8)).save(\n",
    "        \"generated_image.png\")\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--mode\", type=str)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "    parser.add_argument(\"--nice\", dest=\"nice\", action=\"store_true\")\n",
    "    parser.set_defaults(nice=False)\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = get_args()\n",
    "    if args.mode == \"train\":\n",
    "        train(BATCH_SIZE=args.batch_size)\n",
    "    elif args.mode == \"generate\":\n",
    "        generate(BATCH_SIZE=args.batch_size, nice=args.nice)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
