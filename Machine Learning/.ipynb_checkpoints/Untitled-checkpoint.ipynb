{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression 线性回归\n",
    "\n",
    "## Model 模型概述\n",
    "\n",
    "使用线性回归进行建模，假设输出$y$和输入$\\textbf{x}=\\left\\{x_1, x_2, \\cdots, x_n\\right\\}$之间满足如下的线性关系：\n",
    "\n",
    "$$y = \\sum_{i=1}^{n}{x_iw_i}+b=\\textbf{w}^T\\textbf{x}+b$$\n",
    "\n",
    "其中，$\\textbf{w}=\\left\\{w_1, w_2, \\cdots, w_n\\right\\}$.\n",
    "\n",
    "接下来给出线性回归训练的损失函数(loss function)$J(\\textbf{w})$，\n",
    "\n",
    "$$J(\\textbf{w})=\\frac{1}{2}\\sum_{j=1}^{m}{(y^{(j)}-\\hat{y}^{(j)})}^2=\\frac{1}{2}\\sum_{j=1}^{m}{(\\textbf{w}^T\\textbf{x}^{(j)}+b_j-\\hat{y}^{(j)})}^2$$\n",
    "\n",
    "其中$y^{(j)}$是预测的值，$\\hat{y}^{(j)}$是真实值，m代表训练样本数。它代表了预测值与实际值之间的均方误差，线性回归的目的就是寻找参数$\\textbf{w}$使得损失函数最小化。\n",
    "\n",
    "## 一般梯度下降法搜索最小值\n",
    "\n",
    "对于简单的线性回归问题来说，使用一般梯度下降法一定能搜索到最小值。它的\n",
    "\n",
    "步骤如下：\n",
    "\n",
    "1. 计算$J(\\textbf{w})$的梯度，$\\nabla_{\\textbf{w}} J(\\textbf{w})=D\\textbf{w}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real weight: [ 0.16816986  0.52934203  0.59882884  0.08783305  0.79678007]\n",
      "Now epoch: 0\n",
      " weight: [[ 0.36708486  0.5119386   0.49373507  0.35627961  0.49043631]]\n",
      "--------------------\n",
      "Now epoch: 1\n",
      " weight: [[ 0.30951788  0.52553537  0.51002247  0.26245618  0.59901812]]\n",
      "--------------------\n",
      "Now epoch: 2\n",
      " weight: [[ 0.26876445  0.53190842  0.526203    0.20173911  0.66861738]]\n",
      "--------------------\n",
      "Now epoch: 3\n",
      " weight: [[ 0.23997586  0.53452719  0.54089727  0.16247418  0.71344382]]\n",
      "--------------------\n",
      "Now epoch: 4\n",
      " weight: [[ 0.21956466  0.5351766   0.5534426   0.13698404  0.74239395]]\n",
      "--------------------\n",
      "Now epoch: 5\n",
      " weight: [[ 0.20504441  0.53486784  0.56374746  0.12036653  0.76114716]]\n",
      "--------------------\n",
      "Now epoch: 6\n",
      " weight: [[ 0.19468342  0.53415748  0.57199286  0.10948396  0.77333527]]\n",
      "--------------------\n",
      "Now epoch: 7\n",
      " weight: [[ 0.18726998  0.53333886  0.57846742  0.10232243  0.78128516]]\n",
      "--------------------\n",
      "Now epoch: 8\n",
      " weight: [[ 0.18195252  0.53255587  0.58348063  0.09758532  0.78649085]]\n",
      "--------------------\n",
      "Now epoch: 9\n",
      " weight: [[ 0.17813009  0.53186988  0.58732072  0.09443496  0.78991389]]\n",
      "--------------------\n",
      "Now epoch: 10\n",
      " weight: [[ 0.17537703  0.53129865  0.59023745  0.09232814  0.79217477]]\n",
      "--------------------\n",
      "Now epoch: 11\n",
      " weight: [[ 0.17339078  0.53083833  0.59243794  0.09091112  0.79367508]]\n",
      "--------------------\n",
      "Now epoch: 12\n",
      " weight: [[ 0.17195561  0.53047571  0.59408904  0.08995253  0.79467557]]\n",
      "--------------------\n",
      "Now epoch: 13\n",
      " weight: [[ 0.17091727  0.53019472  0.59532238  0.0893003   0.79534614]]\n",
      "--------------------\n",
      "Now epoch: 14\n",
      " weight: [[ 0.17016517  0.52997967  0.59624026  0.08885399  0.79579793]]\n",
      "--------------------\n",
      "Now epoch: 15\n",
      " weight: [[ 0.16961987  0.52981663  0.59692128  0.08854686  0.79610392]]\n",
      "--------------------\n",
      "Now epoch: 16\n",
      " weight: [[ 0.16922416  0.52969394  0.59742526  0.08833438  0.79631227]]\n",
      "--------------------\n",
      "Now epoch: 17\n",
      " weight: [[ 0.16893679  0.52960217  0.59779743  0.08818662  0.79645486]]\n",
      "--------------------\n",
      "Now epoch: 18\n",
      " weight: [[ 0.16872797  0.52953384  0.59807174  0.08808337  0.79655297]]\n",
      "--------------------\n",
      "Now epoch: 19\n",
      " weight: [[ 0.16857614  0.52948316  0.59827362  0.08801089  0.79662079]]\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "常规梯度下降法，每次用一个样本训练，并更新一次权重\n",
    "\"\"\"\n",
    "\n",
    "X = np.random.ranf((100, 5))\n",
    "W_real = np.random.ranf(5)\n",
    "print('Real weight: %s' % (W_real, ))\n",
    "Y = np.dot(W_real, X.T)\n",
    "epoch = 20\n",
    "data_count = X.shape[0]\n",
    "lr = 0.05\n",
    "\n",
    "W = np.zeros((1, 5))\n",
    "\n",
    "for i in range(epoch):\n",
    "    for j in range(data_count):\n",
    "        W = W + lr * (Y[j] - np.dot(X[j,:], W.T)) * X[j,:]\n",
    "    print('Now epoch: %s\\n weight: %s\\n--------------------' % (i, W))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
