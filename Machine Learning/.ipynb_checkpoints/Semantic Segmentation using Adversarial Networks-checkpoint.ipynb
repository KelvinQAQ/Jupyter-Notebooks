{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segementic Segmentation using Adversarial Networks\n",
    "\n",
    "**date**: 25 Nov, 2016\n",
    "\n",
    "**author**: _Pauline Luc, Camille Couprie, Soumith Chintala, Jakob Verbeek_\n",
    "\n",
    "**摘要**：前对抗训练展示出的结果在生成图像模型中是最优的。在这篇文章中，我们提出了一种对抗训练的方法来训练语义分割模型（semantic segmentation models）。我们同时训练了一个对抗网络和卷积语义分割网络，对抗网络用来分辨输入究竟是来自分割网络还是真实分割。我们做这项工作的动力是它能发现网络输出和真实分割之间的高维不一致性。我们的实验表明，在 PASCAL VOC 2012 数据集上，我们的对抗训练方法获得了精度上的提升。\n",
    "\n",
    "## 简介\n",
    "\n",
    "语义分割问题可以被看做是一个稠密的打标签问题（dense labeling problem），即对图像的每一个像素点都预测它所属的类别并给每一个像素都打上相应类别的标签。\n",
    "\n",
    "在本文之前，最优的方法是卷及神经网络方法。CNN有很多结构，但对于其中的多数都存在着**空间连续性不强**的问题，即每个像素点的标记是相互独立的。但实际上，对于一幅图像，相邻像素是高度相关的，导致很多像素被重复地计算所以pixel-by-pixel的方法会大大降低计算效率。\n",
    "\n",
    "文中提到了 Conditional Markov random fields（CRFs）是目前最好的加强空间连续性的方法，结合CNN能获得比一般CNN结构更好的结果。但是文中提到，这样的方法对于高维势（higher-order potentials）是有局限性的。\n",
    "\n",
    "本文想要做的是在提高高阶一致性的同时，不被限制在一个特定类的高阶势中。\n",
    "\n",
    "## 语义分割网络的对抗训练方法\n",
    "\n",
    "本节讲述的是对语义分割网络的对抗训练框架的通用描述，本节还将展示一个我们在试验中使用的一个结构。\n",
    "\n",
    "### 对抗训练\n",
    "\n",
    "我们提出了一个混合损失函数，这个混合损失函数是由两个部分的和组成的。第项是一个多元交叉熵（multi-class cross-entropy），这个部分鼓励分割模型独立地预测每个像素位置所属的正确标签。我们使用$s(x)$来表示$x$所属不同类别的概率图，它的尺寸是$Height \\times Width \\times Class$，输入$x$是一幅RGB图像，尺寸为$Height \\times Width \\times 3$\n",
    "\n",
    "第二项给予一个辅助的对抗卷积网络，这一项的值在对抗网络能够正确分辨出输出是来自ground-truth还是来自分割网络时会很大。我们使用 $a(x,y)\\in [0, 1]$ 是一个标量，表示对抗模型预测$y$是对应$x$的ground-truth的概率，与被预测为是来自分割模型$s(\\cdot)$的概率相反.\n",
    "\n",
    "给定一个长度为 $N$ 的图像数据集 $x_n$ 和对应的标签图 $y_n$ ，我们这样定义损失函数：\n",
    "\n",
    "$$\\mathcal{l}(\\theta_s, \\theta_a)=\\sum_{n=1}^N\\mathcal{l}_{mce}(s(x_n), y_n)-\\lambda\\left[\\mathcal{l}_{bce}(a(x_n, y_n), 1)+\\mathcal{l}_{bce}(a(x_n, s(x_n)), 0)\\right]$$\n",
    "\n",
    "> 我认为这样定义的“损失函数”有些欠妥，因为这样不容易衡量模型整体的训练效果，尽管两个网络是单独、交替训练的。生成模型的效果越好，使函数值减小，对抗模型的效果越好，反而使函数值增大。从而仅凭借总体的损失函数值难以评判模型整体的好坏。\n",
    "> 这里有一个改进的提议，将减号改为加号，这样两个模型的改进都会使函数值减小，从而使得这个总体的损失值在评估模型效果上是有意义的。（待验证）\n",
    "\n",
    "其中，$\\theta_s$, $\\theta_a$分别是分割模型和生成模型的参数，训练的目的是对两组参数寻优，使得与$\\theta_s$有关的项最小，同时使$\\theta_a$有关的部分最大（因为有负号）\n",
    "\n",
    "#### 训练对抗模型\n",
    "\n",
    "loss函数中与$\\theta_a$有关的部分，即对抗模型的损失函数：\n",
    "\n",
    "$$\\sum_{n=1}^N\\mathcal{l}_{bce}(a(x_n, y_n), 1)+\\mathcal{l}_{bce}(a(x_n, s(x_n)), 0)$$\n",
    "\n",
    "训练的目标是最大化第二项（连同负号），即最小化中括号内的部分（即不管负号和学习率$\\lambda$）。在本文的试验中$s(\\cdot)$是一个CNN。本文还用了多种形式的计算来结合输入和对抗网络的感知域，这将在后面介绍到。\n",
    "\n",
    "#### 训练分割模型\n",
    "\n",
    "loss函数中与$\\theta_s$有关的部分，即分割模型的损失函数：\n",
    "\n",
    "$$\\sum_{n=1}^N\\mathcal{l}_{mce}(s(x_n), y_n)-\\lambda\\mathcal{l}_{bce}(a(x_n, s(x_n)), 0)$$\n",
    "\n",
    "在真实的训练中，我们将后半部分替代为$+\\lambda\\mathcal{l}_{bce}(a(x_n, s(x_n)), 1)$，即不寻求使分割模型被对抗模型辨别出来的可能性最小，转而寻求使分割模型使对抗模型误判分割模型的输出是ground-truth的可能性最大。**这样训练的基本原理就在于，若对抗模型正确判别出了一个输入是来自ground-truth还是来自分割模型的输出，就在反馈时给出一个更大的梯度信号，这对于加速训练有很重要的意义**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
